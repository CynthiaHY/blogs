---
layout: post
title: "Jenkins based CI/CD pipeline for Liberty applications"
categories: blog
author_picture: https://avatars3.githubusercontent.com/shamjithantholi
author_github: https://github.com/shamjithantholi
seo-title: TITLE - OpenLiberty.io
seo-description: DESCRIPTION
blog_description: "A high level explanation about how to build a devops pipeline with Jenkins as the core controller component "
open-graph-image: https://openliberty.io/img/twitter_card.jpg
---
= Jenkins based CI/CD pipeline for Liberty applications
Shamjith Antholi https://github.com/shamjithantholi
:imagesdir: /
:url-prefix:
:url-about: /

[#Intro]
== What is DevOps
DevOps is a culture or a process which enables the integration of the software development (dev) and IT operations (ops) which will facilitate the setup of deployment pipeline as well as helps to improve the speed of software delivery. Automation of the entire build, test and deployment of the software applications can be achieved through DevOps process. There are tons of efficient applications and process methodologies available in the market which can be readily integrated to the DevOps pipeline which will help to achieve the desired results. 

== CI/CD and Jenkins
CI/CD are acronym's for contineous integration and contineous delivery/deployment used in DevOps. Contineous integration is a process where developers frequently merge code changes into a source code  repository which automatically start code build, security scan and test execution and deployment. Automation of this process enables organizations to release on a more frequent basis without compromising on quality.

Jenkins is an opensource automation server which helps to automate application code build, test and deployment. Jenkins comes with a hunders of plugins which eases the various devops pipeline tool integrations into the devops pipeline. 

Open Liberty is a lightweight Java runtime for building cloud-native applications and microservices. In this blog, we'll discuss how you can use jenkins in CI/CD pipeline for java based openliberty application build/test/deployment link:https://openliberty.io[OpenLiberty] 

== Getting ready
Below given is a generic architecture of a simple one dimensional devops pipeline (single environment view).

image::/img/blog/liberty-devops-generic-architecture.png[Liberty devops generic architecture diagram ,width=70%,align="center"]

In this blog i will give you a high level overview of using jenkins as a "Code build tool" (with sample codes and required explanations), how you can do the docker image generation and save the same to any docker repository, how to do the deployment to a containerization application (sample commands provied for IBM cloud k8s configuration and general k8s deployment instructions), simple instructions to setup automatic QA job triggering after deployment and finally introducing k8s monitoring tools along with sample k8s commands to check the resource and log status from within the k8s infrastructure itself. Detailed explanation of technologies used in this blog, tool installation, complex devops pipeline design details of multi-environment application deployment are not in the scope of this blog. 

This blog explain the steps to deploy the application on Kubernetes, i am using IBM cloud kubernetes service for deployment (also added IBM cloud connectivity commands), but same steps explained will work for deployment on any other kubernetes service. 

Basic understanding of git, Docker, Kubernetes are a prerequisite for this blog.

The standard Dockerfile which you use in general will not be enough to do the Liberty application deployment on containerized environment, a sample OpenLiberty compliant Dockerfile snipped in given below, please configure it as per your requirement

  FROM icr.io/appcafe/open-liberty:kernel-slim-java8-openj9-ubi
  # Add Liberty server configuration including all necessary features
COPY --chown=1001:0  server.xml /config/
# Modify feature repository (optional)
# A sample is in the 'Getting Required Features' section below
COPY --chown=1001:0 featureUtility.properties /opt/ol/wlp/etc/
# This script will add the requested XML snippets to enable Liberty features and grow image to be fit-for-purpose using featureUtility. 
# Only available in 'kernel-slim'. The 'full' tag already includes all features for convenience.
RUN features.sh
# Add interim fixes (optional)
#COPY --chown=1001:0  interim-fixes /opt/ol/fixes/
.
.
.
.
RUN cp <open-liberty-application>.war /config/dropins/
RUN chmod 755 /config/dropins/<open-liberty-application>.war
RUN chown 1001:0 /config/dropins/<open-liberty-application>.war
WORKDIR /
# This script will add the requested server configurations, apply any interim fixes and populate caches to optimize runtime
RUN configure.sh


=== Installing and configuring jenkins and additional tools ===

Install *jenkins* with all the recommended plugins link:https://www.jenkins.io/doc/book/installing/[install Jenkins], make sure that the following plugins are installed. You can install jenkins on any physical/virtual servers or it can be running as a container on Kubernetes itself.

* Maven
* Pipeline
* Multibranch Scan Webhook Trigger
* Docker
* Kubernetes

If you are using *Helm* to automate the application deployment, install it on the server where jenkins are installed. If you are running jenkins on container, do this installation on the base image used for creating jenkins image. If you are using any slave server to run the jenkins job, install helm on the slave server.

_A note about jenkins slave_

Jenkins builds the Liberty Java code using Jenkins pipeline scripts. The script can run directly on your Jenkins installation (known as Jenkins master) but, if the application is big, you need to use a Jenkins slave agent (good configuration server (or container )). More details about jenkins slave setup are provided at link:https://www.jenkins.io/doc/book/using/using-agents/[Jenkins slave setup], link:https://www.jenkins.io/doc/book/pipeline/syntax/[pipeline code details] 

*Additional tools*

Basic additional tools required on CI/CD pipeline apart from Jenkins are 

* A source code management (SCM) tool like GitHub.

     Provision a public or private github repository (github.com) and checkin your code into it. 
     Create any branching strategy of your choice (example: develop --> qa --> develop branch hierarchy). More details about branch are available here "https://www.atlassian.com/git/tutorials/comparing-workflows/gitflow-workflow" 

* A credential store application like harshicorp vault (optional)

     Your credentials (like dockerhub credentials, artifactory api credentials, IKS api token, github personal access 
     token etc) can be securely saved within Jenkins itself on the page,  
     "http://localhost:8080/credentials/store/system/domain/_/newCredentials++". 
     Consider using external credential store application like vault for better security

* Maven repository and Docker image repository, like artifactory (optional)

     Create an IBM cloud image registry if required by following the steps in 
     "https://cloud.ibm.com/docs/Registry?topic=Registry-getting-started" for pushing the docker images created for 
     deployment. You can use public docker hub registry as well for this.Artifactory is another popular software 
     in the market for this "https://www.jfrog.com/confluence/display/JFROG/Getting+Started+with+Artifactory+as+a+Docker+Registry"

* Vulnerability scanning tools, like Aqua, Trivy, NexusIQ, Sonarqube (optional)

      Vulnerability scan will discover the critical issues in the source code and the open source jar files used 
      in the application. 
      Vulnerability scan can be done on multiple phases, either using CLI commands in jenkins along with the application 
      code build commands or by manually uploading all the jars to the 
      scanning software or by scanning the docker image created for deployment. 
      Most of the scanning softwares do have a recommendation section which we can use to select the correct jar (please 
      verify the database against which those applications are comparing the vulnerability score and take the best decision for your application). 
      Static code analysis can be done based on the code rules made available by the scanning product or you can develop 
      your own custom rules against which you can scan your code before production deployment to find out any critical faults. 
      Softwares like Aqua (docker image scan - "https://www.aquasec.com"), SonarQube (static code analysis - "https://www.sonarqube.org"), NexusIQ (jar scan - "https://help.sonatype.com/iqserver") are popular in market for this. 
      More details are given in the below sections.

* Kubernetes 

     I am using IKS (https://www.ibm.com/ca-en/cloud/kubernetes-service) for this blog, provision the kubernetes 
     cluster on IBM cloud kubernetes service(IKS) , generate IKS API key for CLI connectivity, verify the basic 
     k8s cluster login commands to various clusters or namespaces (like dev cluster, qa cluster etc). 
     You can use any other  kubernetes service of your choice.     

* Logging and monitoring

 Since the cloud application usage are charged based on time used, it's very important to design the use of cloud 
resources in an efficient way. Memory usage stats and application storage plan is important in this perspective 
because the choice of memory/CPU numbers can be set based on this stats, also choice of storage devices and 
its amount allocation also can be selected based on these data.
Also, kubernetes does not store any logs or memory stats permanently. There are applications like dynatrace and 
grafana available in market for storing memory stats permanently and applications like prometheus and splunk in 
market for storing application and cluster log permanently. 
More details about the tools and k8s commands are given in below sections
    

== Jenkins pipeline scripting introduction

It's recommended to adhere to the concept of infrastructure as a code (IaaC) in DevOps pipeline, pipeline scripting based jenkins jobs are a good example of this

You can write link:https://www.jenkins.io/doc/book/pipeline/syntax/[Jenkins pipeline code] in one of the following ways:

1. Writing pipeline code directly in Jenkins (Need to take the backup of this jenkins instance to secure the code)

image::/img/blog/pipeline-code-on-jenkins.png[Pipeline code directly on Jenkins ,width=70%,align="center"]

2. Writing pipeline code on Jenkinsfile (a plain text file) in git and mapping the same into Jenkins. 

image::/img/blog/pipeline-code-on-git.png[Pipeline code on Git ,width=50%,align="center"]

3. If you have specific build/deployment configurations for separate environments, such as dev, staging, and production, you create a separate Jenkinsfile for each environment and store it in that environment specific git repo  branches. 

image::/img/blog/multiple-branch-pipeline-jenkinsview-with-corresponding-gitview.png[Multi branch pipeline setup and corresponding git view,width=75%,align="left"]

Only CLI commands can be used in pipeline code, on the other side, free style and maven type job have the advantage of UI based configuration.
Pipeline code syntax can be found at link:https://www.jenkins.io/doc/pipeline/tour/hello-world/[pipeline syntax]. On jenkins, use this page to generate pipeline code link:http://localhost:8080/job/pipeline_test/pipeline-syntax/[Jenkins] (sample page).

== Code build, packaging and Docker image

You are now ready for testing code build, packaging and generating docker image and push it to any remote docker repository. Remote docker repository is not required if your containerization application is providing the facility of local docker repository(like RedHat OpenShift )

A sample pipeline code for performing code build, packaging and generating docker image and pushing the same to remote docker repository is given below. You can use it by modifying the parameter section (<>)

 pipeline {
     agent any
     stages {
       stage('Build') {
           steps {
                checkout([$class: 'GitSCM', branches: [[name: '*/main']], extensions: [], userRemoteConfigs: [[credentialsId: ‘<git token>, url: 'https://github.com/liberty/app.git']]])
		configFileProvider([configFile('<settings_file.xml>’)]) {
                          sh '''
                                 mvn -U package
                                 docker login <remote-docker-image-repository-url> -u "${USERNAME}" -p “${PASSWORD}”
				  docker tag liberty-$<code identifier>:$<docker image version> <remote-docker-image-repository-url>/<docker-repo-name>/liberty-$<code identifier>:$<docker image version>
 				  docker push <remote-docker-image-repository-url>/<docker-repo-name>/liberty-$<code identifier>:$<docker image version>

                           '''   
                   }}}}}


Following are the parameter used in this example code

* git token: Generate the personal access token from github and paste the same at this location
* settings_file.xml: If you are using special proxy settings files to resolve the dependencies from any private maven repository, you can upload the same to config files page in jenkins and provide the same of the same at this location. Alternatively you can upload the settings file as secure credential files as well. Maven settings file can be generated from link:http://localhost:8080/configfiles/addConfig[generate maven settings file] or use turorials like link:https://www.baeldung.com/maven-settings-xml[generate maven settings file] to generate it
* remote-docker-image-repository-url : Docker image registry/repository url
* USERNAME/PASSWWORD: user name and password to connect to docker registry, this can be saved securely as jenkins cedentials and do the binding of the same to the pipeline job created.

image::/img/blog/jenkins-cred-binding-and-corresponding-param.png[Pipeline credential binding and corresponding param,width=30%,align="center"]

* code identifier: This is optional, a unique docker image identifier
* docker image version: docker image version number, a unique identifier

*important:* There are 2 ways to package the docker image, with or without embedding application code in the docker image. Downloading the code into container at runtime (in entrypoint file) will secure the application code if docker image repository is compromised.    

*Security scan* For static code analysis, we can use SonarQube community edition. Install sonarqube server by either using file startup type from cli downloading the package in link:https://www.sonarqube.org/success-download-community-edition/[SonarQube server install package] or use docker way as explained in link:https://docs.sonarqube.org/latest/setup/get-started-2-minutes/[Sonarqube server install steps]. SonarQube jenkins client setup details are given in this page link:https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-jenkins/[SonarQube client for jenkins]. Features like quality gate on sonarqube can be used to fail the code build in case of not satisfying the required code quality and coverage. Maven build command can be integrated with scan related CLI commands or these can be done on a different pipeline stage. Detailed steps to perform security scan will be available in another blog

For Docker image scan, you can use link:https://hub.docker.com/r/aquasec/trivy/[Docker image scan with trivy]. 

Other licensed options available are link:https://help.sonatype.com/iqserver/getting-started[NexusIQ - jar scan] and link:https://support.aquasec.com/support/solutions/articles/16000112614-aqua-onboarding-guide[Aqua scan]

== Deployment (CLI) 

There are multiple options/tools to trigger the application deployment into Kubernetes from Jenkins (CLI, Helm, Travis CI, Circle CI etc). We will consider only command line (CLI) option here.

Create a new stage in the above given sample pipeline code and write all the required commands between the shell option (sample commands give below)
                           
                           sh '''
                               --> command to connect to IBM cloud (if you are using IKS)
                               --> command to connect to dev/qa/prod cluster
                               --> kubeclt commands to deploy, create service, route etc
                               --> any other commands  
                              '''

-> Maintain all the kubernetes configuration files in the same code repository (you can choose a different repository as well for this) 

image::/img/blog/kubernetes-configuration-files-in-github.png[Kubernetes configuration files in github,width=50%,align="center"]

-> Sample kubernetes deployment file

image::/img/blog/sample-k8s-app-deployment-file.png[Sample kubernetes deployment configuration files in github,width=40%,align="center"]

When code checkout is done for code build into Jenkins, all these  kubernetes configurations files will be downloaded to jenkins workspace, you can run the required IBM cloud/k8s commands to connect to the kubernetes cluster and deploy the application. 

 -> Set the kubernetes context as per the requirement, for example, if we need to deploy into development cluster, 
 then the context should be set to development cluster, for deployment into QA environment, 
 set it into QA context ( this context setting is depending on the design of the cluster)

Sample commands: 

* ibmcloud ks cluster config --cluster <cluster name or id>
* kubectl config current-context
* kubectl create -f deployment.yaml ( simple k8s deployment command )
* kubectl create -f service.yaml ( simple k8s deployment command )
* kubectl create -f route.yaml ( simple k8s deployment command )

All the other required application deployment commands are available in this kubernetes command page which is very straightforward
link:https://kubernetes.io/docs/reference/kubectl/cheatsheet/[Kubernetes sample commands] 

== QA testing options
Apart from running JUnit test cases along with the code build phase, we can configure jenkins and deployment configurations to trigger the functional/integration QA test cases automatically after the deployment in each environment. 

Configure the test cases on jenkins job and test it manually. Create an "Authentication Token" in "Trigger builds remotely" section under "Build Triggers". Trigger this test case from docker "entrypoint" file using remote rest api call using this authentication token as the identifier

Eg: curl -I -u <auth-token> https://<jenkins-host>/job/<job-name>/build?token=<authentication-token>
Note: Auth token can be generated from postman

== Kubernetes monitoring tools

Kubernetes provides commands to check the application/cluster logs and memory/cpu usage through the commands like 

    -> kubectl logs ..
    -> cat /sys/fs/cgroup/cpu/cpuacct.usage (after connecting to k8s pod)
    -> cat /sys/fs/cgroup/memory/memory.usage_in_bytes (after connecting to k8s pod)

For persistence of logs and usage stats, there are sevaral applications available in the market which can be integrated with kubernetes, details about some of those apps are given below

These tools are deployed in kubernetes cluster itself where the application is running and exposed using route and access the gathered details from UI.

* OpenSource :

    -> https://grafana.com/oss/loki/
    -> https://medium.com/nerd-for-tech/logging-at-scale-in-kubernetes-using-grafana-loki-3bb2eb0c0872
    -> https://prometheus.io
    -> https://k21academy.com/docker-kubernetes/prometheus-grafana-monitoring/

* Enterprise :

    -> https://www.splunk.com/en_us/blog/platform/deploy-splunk-enterprise-on-kubernetes-splunk-connect-for-kubernetes-and-splunk-insights-for-containers-beta-part-1.html
    -> https://www.dynatrace.com/support/help/setup-and-configuration/setup-on-container-platforms/kubernetes

== Conclusion
DevOps is a vast ocean and hence i cannot include all details in one blog. Above given details will defenitely help you to get an idea of Jenkins based devops pipeline and will help you to setup a simple pipeline following the steps and explanations. Detailed implementatio of involved components will be covered in a separate blog  

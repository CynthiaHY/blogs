---
layout: post
title: "Using Jenkins for CI/CD with Liberty"
categories: blog
author_picture: https://avatars3.githubusercontent.com/shamjithantholi
author_github: https://github.com/shamjithantholi
seo-title: TITLE - OpenLiberty.io
seo-description: DESCRIPTION
blog_description: "A high level explanation about how to build a devops pipeline with Jenkins as the core controller component "
open-graph-image: https://openliberty.io/img/twitter_card.jpg
---
= Using Jenkins for CI/CD with Liberty
Shamjith Antholi https://github.com/shamjithantholi
:imagesdir: /
:url-prefix:
:url-about: /

[#Intro]
== What is DevOps
DevOps is a culture or a process which enables the integration of the software development (dev) and IT operations (ops) which will facilitate the setup of deployment pipeline as well as helps to improve the speed of software delivery. Automation of the entire build, test and deployment of the software applications can be achieved through DevOps process. There are tons of efficient applications and process methodologies available in the market which can be readily integrated to the DevOps pipeline which will help to achieve the desired results. 

== CI/CD and Jenkins
CI/CD are acronym's for contineous integration and contineous delivery/deployment used in DevOps. Contineous integration is a process where developers frequently merge code changes into a central repository and kick start code build, security scan and test execution using automation or manual methods. Contineous delivery/deployment is a process of application deployment into deployment destination.

Jenkins is an opensource automation server which helps to automate application code build, test and deployment. Jenkins comes with a vast variety of plugins which eases the various devops pipeline tool integrations into the build/deployment pipeline. In this blog, we will focus on how jenkins can be used as a piller of contineous integration and contineous deployment process for java based openliberty application deployment link:https://openliberty.io[OpenLiberty] 

== Getting ready
Below given is a generic architecture of a simple one dimensional devops pipeline (single environment view).

image::/img/blog/liberty-devops-generic-architecture.png[Liberty devops generic architecture diagram ,width=70%,align="center"]

Some of the generic prerequisites to setup devops pipeline are listed below, detail explanation of the setup and configuration process of these tools are not in the scope of this blog.

To start with the tools, setup a *source code management (SCM)* software. We are using GitHub as the SCM tool in this blog. Create a public repository in github.com or create a private github repository for this purpose (please follow the github.com documentation for this, link:https://github.com/git-guides[GitHub] ). Designing an ideal branching strategy is the next step. By default, a master branch will be created on every repository created on github, create additional 2 branches, test and develop in the same order (i.e, checkout master code to test branch, then checkout test  branch to develop branch). You can allow the developers to push the code to "develop" branch and then merge the develop branch code to test branch when ready to deploy the code to test environment(using git pull request). When ready for production deployment, merge the code with master branch using pull git pull request(this is only an example scenario of branching strategy, you can use any strategy of your choice)  

Install *jenkins* with all the recommended plugins link:https://www.jenkins.io/doc/book/installing/[install Jenkins], make sure that the following plugins are installed

* Maven
* Pipeline
* Helm (not a plugin, but install helm on master or any jenkins slave server )
* Docker
* Kubernetes

In general devops pipeline setup cases, we will need a *docker image store* repository to store the docker images generated from the code/Dockerfile  building application (like Jenkins), this remote docker image url will need to be referred in kubernetes deployment configurations later (it's not mandatory to create a private image repository if we can use local image registry like the type we use in openshift). For this purpose, you can   create a custom image registry on IBM cloud. Docker repositories can also be created on tools like artifactory or on other open source softwares like DockerHub, GitLab container registry, Nexus repository etc.  

Credential used in DevOps pipeline (like dockerhub credentials, artifactory api credentials, IKS api token, github personal access token etc) need to stored securely. Jenkins inbuild *credential store* can be used for this purpose. Credentials can be created from link:http://localhost:8080/credentials/store/system/domain/_/newCredentials[Jenkins credentials page] (sample page). If you want to use an external credential store, software's like vault are available in the market. Vault server can be installed on containers or VM and can be integrated with application containers to pull the credentials from server. More installation details are found in link:https://learn.hashicorp.com/vault[vault installation]. For vault-jenkins integration, you can use "HashiCorp Vault" jenkins plugin.

OpenSource jar file *vulnerability scanning* in devops pipeline can be performed using softwares like Aqua (docker image scan) or Sonatype Nexus IQ server (for jar scan).Both are licensed softwares, link:https://support.aquasec.com/support/solutions/articles/16000112614-aqua-onboarding-guide[Aqua setup step], link:https://help.sonatype.com/iqserver/getting-started[NexusIQ setup steps]. link:https://hub.docker.com/r/aquasec/trivy/[Trivy] is an opensource image scanning option, but it works only with the images on dockerhub repository (scanning steps are available in the given link). Static code analysis is also part of vulnerability scan. Sonarqube is the most popular tool used for this, you can do the basic scanning using the opensource version of this software link:https://www.sonarqube.org/downloads/[sonarqube server installation], link:https://docs.sonarqube.org/latest/analysis/scan/sonarscanner-for-jenkins/[sonarqube client setup on jenkins]   
 
Apart from IKS, there are sevaral choices for the *containeirzation* question and we can choose any of them for this pipeline setup purpose. Azure kubernetes service (AKS), Amazon EKS, Google Kubernetes Engine (GKE) etc are some of the other cloud services which we can opt for if required, also we can setup the kubernetes on our own data center. 

If you have the kubernetes cluster ready for connecting with devops pipleine, verify if the *kubernetes context* can be configured/changed correctly on runtime (from any tools, like Jenkins ) to connect to particular cluster/namespace based on the environment choice (you can verify the k8s context detail using the command "kubectl config current-context"). i.e, on run time, if a deployment need to be done to QA cluster or namespace and this choice will be done based on the run time parameters, then kubectl commands should be able to change the k8s context to required destination for proper deployment. 

== Jenkins pipeline script and integrations
After completing the devops tool setup as explained in the previous section, we should start working on creating jenkins pipeline code and integrating the same with Jenkins. This stage can be used to understand some aspects of pipeline concept and jenkins integration, actual code build setup and testing concepts are explained in the next section.  

Create the code build jobs in Jenkins, for adhering to the concept of infrastructure as a code (IaaC), use pipeline or multibranch pipeline type jenkins job for the CI/CD process. Only CLI commands can be used in pipeline code, on the other side, free style and maven type job have the advantage of UI based configuration.
Pipeline code syntax can be found at link:https://www.jenkins.io/doc/pipeline/tour/hello-world/[pipeline syntax]. On jenkins, use this page to generate pipeline code link:http://localhost:8080/job/pipeline_test/pipeline-syntax/[Jenkins] (sample page).

Pipeline code can be directly written on jenkins job or saved on Jenkinsfile in github and map the same onto the newly created jenkins pipeline type job. For using multi branch pipeline job, the plugin "Multibranch Scan Webhook Trigger" need to be installed on jenkins which will help to trigger the mapped jenkins on any change on github code.

Create multiple stages in pipeline code for source code (SCM) checkout, code build, security scan and helm command execution etc. Jenkins job can be executed on jenkins master itself or on containerized slave (setup done using kubernetes pod template) or on virtual servers. The selection of this execution environment are based on the size of the application.

== Code build, packaging and security scan 

You can now ready to start writing the actual code which does everything from code build, application deployment artifact packaging, vulnerability scanning and initiating the application deployment. 

You need to be clear about where you are going to run the code build steps, otherwise, which physical or runtime environment are going to execute the complete pipeline code which does all the steps explained above. We either can run everything on jenkins master itself or we can run in on a special server or container called "jenkins slave" on pipeline sample code - you can see the code snippet about slaves as "node('slaveNode1')" in the pipeline code examples). Detailed  slave setup steps are not in the scope of this blog (you can run sample liberty application code on jenkins master itself). More details about jenkins slave setup are provided at link:https://www.jenkins.io/doc/book/using/using-agents/[Jenkins slave setup],     

After declaring the jenkins slave label and other variables in the beginning of pipeline code, create a pipeline "stage" for code checkout into the jenkins workspace, and then initiate the code build using maven commands (You can use single or multiple pipeline stages for these activities). At java code build stage, we may have dependent jar files hosted on public maven repository or in private maven repositories (maven repos created in softwares like nexus or artifactory). Special proxy settings files are required to be configured on Jenkins to resolve the dependencies from any private maven repository. Maven settings file can be generated from link:http://localhost:8080/configfiles/addConfig[generate maven settings file] or use turorials like link:https://www.baeldung.com/maven-settings-xml[generate maven settings file] to generate it, in case of artifactory, you can directly doenload it from maven repository home page. Details in above given examples like repository url, mirror settings, credential settings etc in the maven settings file are self explanatory. Upload this maven settings files to jenkins"managed files" or as secret files page ( please note that special settings are not required if your company firewall allows to resolve the dependencies from maven central repository). This special settings files could be requirement for storing the generated application war file to a private repository as well(details explained below)

After the code build and unit test execution (we can enable and disable unit test execution through the tags configured on application pom.xml), liberty application code should be packaged to .war file. You can consider the common practices like persistent storage of code package in nexus/artifactory because of various reasons like organizations compliance requirements or to directly download it to docker container while deployment thus by avoiding the risk of exposing application code in case of a compromized docker image. Application jar run time upload to private nexus/artifactory can be done on runtime using distributionmanagement tag in maven pom (also need special settings file as explained above).  

Running automatic security scan of source code and dependency jars along with every code build is a good practice which can be implemented as part of CI/CD pipeline which ensures the security of the every version of deployed application. Static code analysis and opensource jar scan should be completed before proceeding to deployment. Use the steps explained in the tool setup stage to complete all security scans from jenkins on run time. Features like quality gate on sonarqube can be used to fail the code build in case of not satisfying the required code quality and coverage. Maven build command can be integrated with scan related CLI commands or these can be done on a different pipeline stage.

Optionally, when code packaging is completed and ready for deployment, the current branch of code can be added to a git tag for any rebuilding purpose. This can be inititad from jenkins itself.  

== Docker image
When the appliction packaging process is completed, next stage should be to manage the docker image generation and its storage. Docker is a prerequistite for this phase (through the jenkins plugin or directly installing on jenkins master node).

Docker image can be generated by running CLI command on the directory where the "Dockerfile" is available (command is give below - to run through pipeline code in jenkins). When the docker build is successfull, an image is created in the local docker repository.  

* docker build -t <docker-image-name>:<version> --build-arg <arg-name>=<arg-value> .

Below given is a glimpse of activities like code checkout, code build, deployment artifact storage, docker image build. In this example, code package is directly embedded into the docker image which is easy though but not recommended. 

image::/img/blog/pipeline-code-example.png[pipeline code example ,width=70%,align="center"]

Next step is to push this local image to a remote repository from where the IBM cloud Kubernetes service can pull this for creating the containers. 

Some helpful cli commands to use in Jenkins are given below (use any Jenkins plugins for the same if available):

*login to the private docker repository*
* docker login <repository host name> -u "${USERNAME}" -p "${PASSWORD}"

*tag the local docker image to remote repo url*
* docker tag <docker-image-name>:<version> <repository host name>/<repository name>/<docker-image-name>:<version>

*push the docker image to remote docker repository*
* docker push <repository host name>/<repository name>/<docker-image-name>:<version>

If cloud authentication and cluster selection is required, use the API key authentication method

* ibmcloud login --apikey <ibm cloud api key> -g <ibm cloud resource group>

== Deployment (CLI and Helm) 
Helm is a good option to facilitate the application deployment on the cloud platform, it eases deployment/maintenance steps and hence highly recommended. But we can do the application deployment on kubernetes using CLI commands directly from Jenkins shell or pipeline stages. 

=== CLI Deployment
When a Docker image is generated and saved on a repositories like K8s/OCP registry, IBM cloud remote registry, artifactory etc, then the docker deployment is very straightforward using kubernetes CLI commands. Either you can generate new image tag on every docker build and update this new name/tag on the deployment yaml file on GitHub (using git push) or you can depend on a single image name/tag for a particular feature release and change it to new on every subsequent release (This change can be done only on the current jenkins workspace file as well if not required to save the information on the github for reference purpose, also if multiple repositories are used for code and container configurations, this push method is helpful).  

As explained in the earlier section, after the kubernetes context is set to the required environment, run the kubectl commands to deploy and components like deployments, services, route, serviceaccount, secrets etc. The yaml files should be already available in the current Jenkins workspace downloaded as explained in the earlier stage (if code and container are part of same repository).   

link:https://kubernetes.io/docs/reference/kubectl/cheatsheet/[Kubernetes sample commands] 

=== Helm deployment
In this stage, we are ready to start the application deployment using Helm link:https://helm.sh/docs/helm/helm/[Helm]. Helm is already available from jenkins server (or on any attached jenkins slave - if we are using virtual machine as the slave, make sure helm is installed on that server and available for all users, if containerized slave are used, make sure the helm installation is done through the dockerfile of the attached image to the slave)

All the deployment related configirations, like, Pod, deployment, service should be completed and checked into github prior to appliction deployment trigger in the helm chart directory link:https://helm.sh/docs/helm/helm_create/[Helm create]

Run the "helm install" or "helm upgrade" from Jenkins shell or pipeline code to create the resources in the kubernetes cluster. Maintain all the helm resources in a separate folder in the git repository and make the modifications as per the requirement.

The name of the new docker image generated on the docker build can be updated on the helm file on run time (if you are adopting to this run time image name change strategy), you can use the "Git Push Plugin" for this purpose on Jenkins. 

Some helpful cli commands for using in Jenkins are given below

* ibmcloud plugin install container-service
* ibmcloud config --check-version=false
* ibmcloud ks cluster config --cluster <ibm cloud cluster id>
* helm uninstall <release name> -n <namespace>
* helm install <release name> . --namespace <namespace>

link:https://phoenixnap.com/kb/helm-commands-cheat-sheet[Helm commands]

Use the kubectl commands to  check the status of deployment or go to the kubernetes dashboard and check the status of the deployment

image::/img/blog/K8S-dashboard.png[Kubernetes dashboard example ,width=70%,align="center"]

== QA testing options
Apart from running JUnit test cases along with the code build phase, we can configure jenkins and deployment configurations to trigger the funtional/integration QA test cases automatically after the deployment in each environment. 

Configure the test cases on jenkins job and test it manually. Create an "Authentication Token" in "Trigger builds remotely" section under "Build Triggers". Trigger this test case from docker "entrypoint" file using remote rest api call using this authentication token as the identifier

Eg: curl -I -u <auth-token> https://<jenkins-host>/job/<job-name>/build?token=<authentication-token>
Note: Auth token can be generated from postman

== Kubernetes monitoring tools
Several enterprise and open source options are available in market for kubernetes cluster resource monitoring and log monitoring. Some working example resources are given below. 

* OpenSource :

    -> https://grafana.com/oss/loki/
    -> https://medium.com/nerd-for-tech/logging-at-scale-in-kubernetes-using-grafana-loki-3bb2eb0c0872
    -> https://prometheus.io
    -> https://k21academy.com/docker-kubernetes/prometheus-grafana-monitoring/

* Enterprise :

    -> https://www.splunk.com/en_us/blog/platform/deploy-splunk-enterprise-on-kubernetes-splunk-connect-for-kubernetes-and-splunk-insights-for-containers-beta-part-1.html
    -> https://www.dynatrace.com/support/help/setup-and-configuration/setup-on-container-platforms/kubernetes


